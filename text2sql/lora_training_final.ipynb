{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-20T19:55:55.777200Z",
     "iopub.status.busy": "2025-05-20T19:55:55.776584Z",
     "iopub.status.idle": "2025-05-20T19:56:17.909510Z",
     "shell.execute_reply": "2025-05-20T19:56:17.908757Z",
     "shell.execute_reply.started": "2025-05-20T19:55:55.777162Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: rouge-score in /usr/local/lib/python3.11/dist-packages (0.1.2)\n",
      "Requirement already satisfied: absl-py in /usr/local/lib/python3.11/dist-packages (from rouge-score) (1.4.0)\n",
      "Requirement already satisfied: nltk in /usr/local/lib/python3.11/dist-packages (from rouge-score) (3.9.1)\n",
      "Requirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (from rouge-score) (1.26.4)\n",
      "Requirement already satisfied: six>=1.14.0 in /usr/local/lib/python3.11/dist-packages (from rouge-score) (1.17.0)\n",
      "Requirement already satisfied: click in /usr/local/lib/python3.11/dist-packages (from nltk->rouge-score) (8.1.8)\n",
      "Requirement already satisfied: joblib in /usr/local/lib/python3.11/dist-packages (from nltk->rouge-score) (1.5.0)\n",
      "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.11/dist-packages (from nltk->rouge-score) (2024.11.6)\n",
      "Requirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (from nltk->rouge-score) (4.67.1)\n",
      "Requirement already satisfied: mkl_fft in /usr/local/lib/python3.11/dist-packages (from numpy->rouge-score) (1.3.8)\n",
      "Requirement already satisfied: mkl_random in /usr/local/lib/python3.11/dist-packages (from numpy->rouge-score) (1.2.4)\n",
      "Requirement already satisfied: mkl_umath in /usr/local/lib/python3.11/dist-packages (from numpy->rouge-score) (0.1.1)\n",
      "Requirement already satisfied: mkl in /usr/local/lib/python3.11/dist-packages (from numpy->rouge-score) (2025.1.0)\n",
      "Requirement already satisfied: tbb4py in /usr/local/lib/python3.11/dist-packages (from numpy->rouge-score) (2022.1.0)\n",
      "Requirement already satisfied: mkl-service in /usr/local/lib/python3.11/dist-packages (from numpy->rouge-score) (2.4.1)\n",
      "Requirement already satisfied: intel-openmp<2026,>=2024 in /usr/local/lib/python3.11/dist-packages (from mkl->numpy->rouge-score) (2024.2.0)\n",
      "Requirement already satisfied: tbb==2022.* in /usr/local/lib/python3.11/dist-packages (from mkl->numpy->rouge-score) (2022.1.0)\n",
      "Requirement already satisfied: tcmlib==1.* in /usr/local/lib/python3.11/dist-packages (from tbb==2022.*->mkl->numpy->rouge-score) (1.3.0)\n",
      "Requirement already satisfied: intel-cmplr-lib-rt in /usr/local/lib/python3.11/dist-packages (from mkl_umath->numpy->rouge-score) (2024.2.0)\n",
      "Requirement already satisfied: intel-cmplr-lib-ur==2024.2.0 in /usr/local/lib/python3.11/dist-packages (from intel-openmp<2026,>=2024->mkl->numpy->rouge-score) (2024.2.0)\n"
     ]
    }
   ],
   "source": [
    "!pip install -q -U transformers\n",
    "!pip install -q -U accelerate\n",
    "!pip install -q -U datasets\n",
    "!pip install -q -U peft\n",
    "!pip install -q -i https://pypi.org/simple/ bitsandbytes\n",
    "!pip install -q -U trl\n",
    "!pip install rouge-score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-20T19:56:17.913017Z",
     "iopub.status.busy": "2025-05-20T19:56:17.912740Z",
     "iopub.status.idle": "2025-05-20T19:56:18.303056Z",
     "shell.execute_reply": "2025-05-20T19:56:18.302516Z",
     "shell.execute_reply.started": "2025-05-20T19:56:17.912989Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "from huggingface_hub import login\n",
    "\n",
    "hf_token = \"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\"\n",
    "\n",
    "login(hf_token)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-20T19:56:18.303919Z",
     "iopub.status.busy": "2025-05-20T19:56:18.303725Z",
     "iopub.status.idle": "2025-05-20T19:56:18.307840Z",
     "shell.execute_reply": "2025-05-20T19:56:18.307244Z",
     "shell.execute_reply.started": "2025-05-20T19:56:18.303903Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0\"\n",
    "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-20T19:56:18.309817Z",
     "iopub.status.busy": "2025-05-20T19:56:18.309632Z",
     "iopub.status.idle": "2025-05-20T19:56:18.319314Z",
     "shell.execute_reply": "2025-05-20T19:56:18.318674Z",
     "shell.execute_reply.started": "2025-05-20T19:56:18.309801Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-20T20:14:55.375212Z",
     "iopub.status.busy": "2025-05-20T20:14:55.374888Z",
     "iopub.status.idle": "2025-05-20T20:14:55.383386Z",
     "shell.execute_reply": "2025-05-20T20:14:55.382646Z",
     "shell.execute_reply.started": "2025-05-20T20:14:55.375182Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "transformers==4.52.0\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "from tqdm import tqdm\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "import transformers\n",
    "from transformers import (AutoModelForCausalLM,\n",
    "                          AutoTokenizer,\n",
    "                          BitsAndBytesConfig,\n",
    "                          TrainingArguments, # Note: SFTConfig from TRL is used later\n",
    "                          pipeline,\n",
    "                          logging)\n",
    "\n",
    "# Explicitly import Gemma3ForCausalLM\n",
    "from transformers.models.gemma3 import Gemma3ForCausalLM\n",
    "\n",
    "from datasets import Dataset\n",
    "from peft import LoraConfig, PeftConfig, PeftModel\n",
    "\n",
    "import bitsandbytes as bnb\n",
    "\n",
    "from sklearn.metrics import (accuracy_score,\n",
    "                             classification_report,\n",
    "                             confusion_matrix)\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Check transformers version\n",
    "print(f\"transformers=={transformers.__version__}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-20T20:04:50.247018Z",
     "iopub.status.busy": "2025-05-20T20:04:50.246433Z",
     "iopub.status.idle": "2025-05-20T20:04:50.251732Z",
     "shell.execute_reply": "2025-05-20T20:04:50.250886Z",
     "shell.execute_reply.started": "2025-05-20T20:04:50.246993Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using compute dtype torch.float16\n"
     ]
    }
   ],
   "source": [
    "# Determine optimal computation dtype based on GPU capability\n",
    "# Use bfloat16 if Compute Capability >= 8.0, otherwise float16\n",
    "compute_dtype = torch.bfloat16 if torch.cuda.get_device_capability()[0] >= 8 else torch.float16\n",
    "print(f\"Using compute dtype {compute_dtype}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-20T20:04:51.424101Z",
     "iopub.status.busy": "2025-05-20T20:04:51.423504Z",
     "iopub.status.idle": "2025-05-20T20:04:51.428492Z",
     "shell.execute_reply": "2025-05-20T20:04:51.427702Z",
     "shell.execute_reply.started": "2025-05-20T20:04:51.424074Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CUDA is available. Using GPU.\n"
     ]
    }
   ],
   "source": [
    "if torch.cuda.is_available():\n",
    "    device = torch.device('cuda')\n",
    "    print('CUDA is available. Using GPU.')\n",
    "else:\n",
    "    device = torch.device('cpu')\n",
    "    print('CUDA is not available. Using CPU.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-20T20:04:52.205558Z",
     "iopub.status.busy": "2025-05-20T20:04:52.205234Z",
     "iopub.status.idle": "2025-05-20T20:04:52.209090Z",
     "shell.execute_reply": "2025-05-20T20:04:52.208406Z",
     "shell.execute_reply.started": "2025-05-20T20:04:52.205538Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "GEMMA_PATH = \"/kaggle/input/gemma-3/transformers/gemma-3-1b-it/1\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-20T20:05:16.625115Z",
     "iopub.status.busy": "2025-05-20T20:05:16.624845Z",
     "iopub.status.idle": "2025-05-20T20:05:20.858169Z",
     "shell.execute_reply": "2025-05-20T20:05:20.857383Z",
     "shell.execute_reply.started": "2025-05-20T20:05:16.625096Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    GEMMA_PATH,\n",
    "    torch_dtype=compute_dtype,\n",
    "    attn_implementation=\"eager\", # Specify attention implementation\n",
    "    low_cpu_mem_usage=True,      # Reduces CPU RAM usage during loading\n",
    ").to(device)\n",
    "\n",
    "max_seq_length = 8192 # Gemma 3 supports long contexts\n",
    "\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\n",
    "    GEMMA_PATH,\n",
    "    max_seq_length=max_seq_length,\n",
    "    device_map=device # Map tokenizer operations if relevant (less common)\n",
    ")\n",
    "\n",
    "EOS_TOKEN = tokenizer.eos_token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-20T20:00:32.480426Z",
     "iopub.status.busy": "2025-05-20T20:00:32.480169Z",
     "iopub.status.idle": "2025-05-20T20:00:32.487404Z",
     "shell.execute_reply": "2025-05-20T20:00:32.486686Z",
     "shell.execute_reply.started": "2025-05-20T20:00:32.480408Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model is on GPU: True\n"
     ]
    }
   ],
   "source": [
    "# Check if all model parameters are on the CUDA device\n",
    "is_on_gpu = all(param.device.type == 'cuda' for param in model.parameters())\n",
    "print(\"Model is on GPU:\", is_on_gpu)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-20T20:00:32.488655Z",
     "iopub.status.busy": "2025-05-20T20:00:32.488439Z",
     "iopub.status.idle": "2025-05-20T20:00:32.502716Z",
     "shell.execute_reply": "2025-05-20T20:00:32.502027Z",
     "shell.execute_reply.started": "2025-05-20T20:00:32.488637Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def format_training_example(row) -> str:\n",
    "    return f\"\"\"<|system|>\n",
    "Ты — интеллектуальный текстовый помощник, преобразующий команды пользователя в корректные SQL-запросы на языке SQLite.\n",
    "\n",
    "Ты всегда работаешь с таблицей `transactions`, имеющей следующую структуру:\n",
    "\n",
    "- id (INTEGER, PRIMARY KEY)\n",
    "- user_id (TEXT)\n",
    "- type (TEXT: 'income' или 'expense')\n",
    "- category (TEXT)\n",
    "- amount (REAL)\n",
    "- date (TIMESTAMP, по умолчанию date('now', 'localtime'))\n",
    "\n",
    "Описание:\n",
    "- `income` — это доход\n",
    "- `expense` — это трата\n",
    "\n",
    "Твоя задача — по команде пользователя с его ID сгенерировать корректный SQL-запрос к этой таблице.\n",
    "\n",
    "Важно:\n",
    "- Используй только SQLite-синтаксис, без пояснений.\n",
    "- Не пиши комментарии.\n",
    "- Не используй несуществующие поля (например, `note`).\n",
    "- Поле `date` можно указывать через date('now', '-N day') или опустить (будет по умолчанию).\n",
    "\n",
    "Пример:\n",
    "---\n",
    "Пользователь с ID user_1 дал команду:\n",
    "\"Добавь трату 500 рублей на еду вчера\"\n",
    "\n",
    "Ответ:\n",
    "INSERT INTO transactions (user_id, type, category, amount, date)\n",
    "VALUES ('user_1', 'expense', 'еда', 500, date('now', '-1 day'));\n",
    "---\n",
    "</s>\n",
    "<|user|>\n",
    "Пользователь с ID {row[\"user_id\"]} дал команду:\n",
    "\"{row[\"user_command\"]}\"\n",
    "</s>\n",
    "<|assistant|>\n",
    "{row[\"gold_sql\"]}\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-20T19:58:16.392629Z",
     "iopub.status.busy": "2025-05-20T19:58:16.391920Z",
     "iopub.status.idle": "2025-05-20T19:58:16.398936Z",
     "shell.execute_reply": "2025-05-20T19:58:16.398102Z",
     "shell.execute_reply.started": "2025-05-20T19:58:16.392595Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def format_test_example(row) -> str:\n",
    "    return f\"\"\"<|system|>\n",
    "Ты — интеллектуальный текстовый помощник, преобразующий команды пользователя в корректные SQL-запросы на языке SQLite.\n",
    "\n",
    "Ты всегда работаешь с таблицей `transactions`, имеющей следующую структуру:\n",
    "\n",
    "- id (INTEGER, PRIMARY KEY)\n",
    "- user_id (TEXT)\n",
    "- type (TEXT: 'income' или 'expense')\n",
    "- category (TEXT)\n",
    "- amount (REAL)\n",
    "- date (TIMESTAMP, по умолчанию date('now', 'localtime'))\n",
    "\n",
    "Описание:\n",
    "- `income` — это доход\n",
    "- `expense` — это трата\n",
    "\n",
    "Твоя задача — по команде пользователя с его ID сгенерировать корректный SQL-запрос к этой таблице.\n",
    "\n",
    "Важно:\n",
    "- Используй только SQLite-синтаксис, без пояснений.\n",
    "- Не пиши комментарии.\n",
    "- Не используй несуществующие поля (например, `note`).\n",
    "- Поле `date` можно указывать через date('now', '-N day') или опустить (будет по умолчанию).\n",
    "\n",
    "Пример:\n",
    "---\n",
    "Пользователь с ID user_1 дал команду:\n",
    "\"Добавь трату 500 рублей на еду вчера\"\n",
    "\n",
    "Ответ:\n",
    "INSERT INTO transactions (user_id, type, category, amount, date)\n",
    "VALUES ('user_1', 'expense', 'еда', 500, date('now', '-1 day'));\n",
    "---\n",
    "</s>\n",
    "<|user|>\n",
    "Пользователь с ID {row[\"user_id\"]} дал команду:\n",
    "\"{row[\"user_command\"]}\"\n",
    "</s>\n",
    "<|assistant|>\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-20T19:58:18.141641Z",
     "iopub.status.busy": "2025-05-20T19:58:18.141364Z",
     "iopub.status.idle": "2025-05-20T19:58:18.187056Z",
     "shell.execute_reply": "2025-05-20T19:58:18.186292Z",
     "shell.execute_reply.started": "2025-05-20T19:58:18.141622Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "train_df = pd.read_csv(\"/kaggle/input/fin-ass-data/fin_ass_train.csv\")\n",
    "eval_df = pd.read_csv(\"/kaggle/input/fin-ass-data/fin_ass_eval.csv\")\n",
    "test_df = pd.read_csv(\"/kaggle/input/fin-ass-data/fin_ass_test.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-20T19:58:19.080348Z",
     "iopub.status.busy": "2025-05-20T19:58:19.079537Z",
     "iopub.status.idle": "2025-05-20T19:58:19.092327Z",
     "shell.execute_reply": "2025-05-20T19:58:19.091581Z",
     "shell.execute_reply.started": "2025-05-20T19:58:19.080314Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>user_id</th>\n",
       "      <th>user_command</th>\n",
       "      <th>gold_sql</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>user_4</td>\n",
       "      <td>Удалите мои последние траты на развлечения</td>\n",
       "      <td>DELETE FROM transactions\\nWHERE user_id = 'use...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>user_4</td>\n",
       "      <td>Покажи мои траты на фриланс между '2024-02-01'...</td>\n",
       "      <td>SELECT * FROM transactions\\nWHERE user_id = 'u...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>user_1</td>\n",
       "      <td>Удалите мои последние траты на развлечения</td>\n",
       "      <td>DELETE FROM transactions\\nWHERE user_id = 'use...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>user_1</td>\n",
       "      <td>Покажи мои расходы по категориям за последние ...</td>\n",
       "      <td>SELECT category, SUM(amount) FROM transactions...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>user_5</td>\n",
       "      <td>Добавь трату 20938 рублей на зарплата 7 дней н...</td>\n",
       "      <td>INSERT INTO transactions (user_id, type, categ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  user_id                                       user_command  \\\n",
       "0  user_4         Удалите мои последние траты на развлечения   \n",
       "1  user_4  Покажи мои траты на фриланс между '2024-02-01'...   \n",
       "2  user_1         Удалите мои последние траты на развлечения   \n",
       "3  user_1  Покажи мои расходы по категориям за последние ...   \n",
       "4  user_5  Добавь трату 20938 рублей на зарплата 7 дней н...   \n",
       "\n",
       "                                            gold_sql  \n",
       "0  DELETE FROM transactions\\nWHERE user_id = 'use...  \n",
       "1  SELECT * FROM transactions\\nWHERE user_id = 'u...  \n",
       "2  DELETE FROM transactions\\nWHERE user_id = 'use...  \n",
       "3  SELECT category, SUM(amount) FROM transactions...  \n",
       "4  INSERT INTO transactions (user_id, type, categ...  "
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-20T19:58:19.525710Z",
     "iopub.status.busy": "2025-05-20T19:58:19.525407Z",
     "iopub.status.idle": "2025-05-20T19:58:19.530571Z",
     "shell.execute_reply": "2025-05-20T19:58:19.529692Z",
     "shell.execute_reply.started": "2025-05-20T19:58:19.525690Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train df shape: (8000, 3)\n",
      "Eval df shape: (1000, 3)\n",
      "Test df shape: (1000, 3)\n"
     ]
    }
   ],
   "source": [
    "print(f\"Train df shape: {train_df.shape}\")\n",
    "print(f\"Eval df shape: {eval_df.shape}\")\n",
    "print(f\"Test df shape: {test_df.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-20T19:58:20.575080Z",
     "iopub.status.busy": "2025-05-20T19:58:20.574256Z",
     "iopub.status.idle": "2025-05-20T19:58:20.648252Z",
     "shell.execute_reply": "2025-05-20T19:58:20.647516Z",
     "shell.execute_reply.started": "2025-05-20T19:58:20.575048Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "train_df[\"text\"] = train_df.apply(format_training_example, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-20T19:58:21.665347Z",
     "iopub.status.busy": "2025-05-20T19:58:21.664572Z",
     "iopub.status.idle": "2025-05-20T19:58:21.678374Z",
     "shell.execute_reply": "2025-05-20T19:58:21.677610Z",
     "shell.execute_reply.started": "2025-05-20T19:58:21.665316Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "eval_df[\"text\"] = eval_df.apply(format_training_example, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-20T19:58:21.926614Z",
     "iopub.status.busy": "2025-05-20T19:58:21.926319Z",
     "iopub.status.idle": "2025-05-20T19:58:21.940796Z",
     "shell.execute_reply": "2025-05-20T19:58:21.939918Z",
     "shell.execute_reply.started": "2025-05-20T19:58:21.926593Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "test_df[\"text\"] = test_df.apply(format_test_example, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-20T19:58:23.271206Z",
     "iopub.status.busy": "2025-05-20T19:58:23.270862Z",
     "iopub.status.idle": "2025-05-20T19:58:23.275736Z",
     "shell.execute_reply": "2025-05-20T19:58:23.274917Z",
     "shell.execute_reply.started": "2025-05-20T19:58:23.271177Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# def predict_sql(df, model, tokenizer, device=\"cuda\", max_new_tokens=128, temperature=0.0):\n",
    "#     \"\"\"Генерация SQL-запросов по текстовому промпту в формате ChatML.\"\"\"\n",
    "#     predicted_sql = []\n",
    "\n",
    "#     model.eval()\n",
    "\n",
    "#     for i in tqdm(range(len(df)), desc=\"Генерация SQL\"):\n",
    "#         prompt = df.iloc[i][\"text\"]\n",
    "\n",
    "#         # Токенизация\n",
    "#         input_ids = tokenizer(prompt, return_tensors=\"pt\").to(device)\n",
    "\n",
    "#         # Генерация\n",
    "#         with torch.no_grad():\n",
    "#             outputs = model.generate(\n",
    "#                 **input_ids,\n",
    "#                 max_new_tokens=max_new_tokens,\n",
    "#                 temperature=temperature,\n",
    "#                 pad_token_id=tokenizer.eos_token_id,\n",
    "#                 eos_token_id=tokenizer.eos_token_id,\n",
    "#                 do_sample=False\n",
    "#             )\n",
    "\n",
    "#         # Декодирование всего текста\n",
    "#         decoded = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "\n",
    "#         # Извлекаем только SQL часть (после <|assistant|>)\n",
    "#         if \"<|assistant|>\" in decoded:\n",
    "#             predicted = decoded.split(\"<|assistant|>\")[-1].strip()\n",
    "#         else:\n",
    "#             predicted = decoded.strip()\n",
    "\n",
    "#         predicted_sql.append(predicted)\n",
    "\n",
    "#     return predicted_sql"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-20T19:58:23.500366Z",
     "iopub.status.busy": "2025-05-20T19:58:23.500048Z",
     "iopub.status.idle": "2025-05-20T19:58:23.503874Z",
     "shell.execute_reply": "2025-05-20T19:58:23.503093Z",
     "shell.execute_reply.started": "2025-05-20T19:58:23.500344Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# test_predicted = predict_sql(eval_df=test_df.head(10), model=model, tokenizer=tokenizer)\n",
    "# test_predicted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-20T19:58:23.974485Z",
     "iopub.status.busy": "2025-05-20T19:58:23.974167Z",
     "iopub.status.idle": "2025-05-20T19:58:23.978471Z",
     "shell.execute_reply": "2025-05-20T19:58:23.977661Z",
     "shell.execute_reply.started": "2025-05-20T19:58:23.974461Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# import re\n",
    "\n",
    "# def clean_sql(text):\n",
    "#     \"\"\"\n",
    "#     Удаляет обёртки ```sql ... ``` и обрезает по первой строке, если нужно.\n",
    "#     \"\"\"\n",
    "#     if not isinstance(text, str):\n",
    "#         return \"\"\n",
    "\n",
    "#     # Убираем блоки ```sql и ```\n",
    "#     cleaned = re.sub(r\"```sql\\s*\", \"\", text, flags=re.IGNORECASE)\n",
    "#     cleaned = re.sub(r\"```\", \"\", cleaned)\n",
    "#     return cleaned.strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-20T19:58:25.126040Z",
     "iopub.status.busy": "2025-05-20T19:58:25.125755Z",
     "iopub.status.idle": "2025-05-20T19:58:25.129883Z",
     "shell.execute_reply": "2025-05-20T19:58:25.129007Z",
     "shell.execute_reply.started": "2025-05-20T19:58:25.126019Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# clean_predicts = list(map(clean_sql, test_predicted))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-20T19:58:25.419128Z",
     "iopub.status.busy": "2025-05-20T19:58:25.418856Z",
     "iopub.status.idle": "2025-05-20T19:58:25.422595Z",
     "shell.execute_reply": "2025-05-20T19:58:25.421745Z",
     "shell.execute_reply.started": "2025-05-20T19:58:25.419108Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# answers = test_df.head(10)[\"gold_sql\"].tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-20T19:58:25.815906Z",
     "iopub.status.busy": "2025-05-20T19:58:25.815339Z",
     "iopub.status.idle": "2025-05-20T19:58:25.819842Z",
     "shell.execute_reply": "2025-05-20T19:58:25.819099Z",
     "shell.execute_reply.started": "2025-05-20T19:58:25.815880Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# from rouge_score import rouge_scorer\n",
    "# import numpy as np\n",
    "\n",
    "# def compute_rouge_scores(predictions, references, use_stemmer=True):\n",
    "#     scorer = rouge_scorer.RougeScorer([\"rouge1\", \"rouge2\", \"rougeL\"], use_stemmer=use_stemmer)\n",
    "\n",
    "#     scores = {\"rouge1\": [], \"rouge2\": [], \"rougeL\": []}\n",
    "\n",
    "#     for pred, ref in zip(predictions, references):\n",
    "#         result = scorer.score(ref, pred)\n",
    "#         for key in scores:\n",
    "#             scores[key].append(result[key].fmeasure)\n",
    "\n",
    "#     return {\n",
    "#         \"ROUGE-1\": round(np.mean(scores[\"rouge1\"]), 4),\n",
    "#         \"ROUGE-2\": round(np.mean(scores[\"rouge2\"]), 4),\n",
    "#         \"ROUGE-L\": round(np.mean(scores[\"rougeL\"]), 4),\n",
    "#     }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-20T19:58:27.681592Z",
     "iopub.status.busy": "2025-05-20T19:58:27.681311Z",
     "iopub.status.idle": "2025-05-20T19:58:27.685374Z",
     "shell.execute_reply": "2025-05-20T19:58:27.684538Z",
     "shell.execute_reply.started": "2025-05-20T19:58:27.681571Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# compute_rouge_scores(clean_predicts, answers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-20T19:58:27.975658Z",
     "iopub.status.busy": "2025-05-20T19:58:27.975378Z",
     "iopub.status.idle": "2025-05-20T19:58:28.081200Z",
     "shell.execute_reply": "2025-05-20T19:58:28.080466Z",
     "shell.execute_reply.started": "2025-05-20T19:58:27.975638Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "train_dataset = Dataset.from_pandas(train_df[[\"text\"]])\n",
    "eval_dataset = Dataset.from_pandas(eval_df[[\"text\"]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-20T20:06:37.610094Z",
     "iopub.status.busy": "2025-05-20T20:06:37.609814Z",
     "iopub.status.idle": "2025-05-20T20:06:37.614488Z",
     "shell.execute_reply": "2025-05-20T20:06:37.613807Z",
     "shell.execute_reply.started": "2025-05-20T20:06:37.610073Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "from transformers import TrainingArguments\n",
    "from peft import LoraConfig, get_peft_model\n",
    "\n",
    "# LoRA Configuration\n",
    "peft_config = LoraConfig(\n",
    "    lora_alpha=16,                           # Scaling factor for LoRA\n",
    "    lora_dropout=0.05,                       # Add slight dropout for regularization\n",
    "    r=64,                                    # Rank of the LoRA update matrices\n",
    "    bias=\"none\",                             # No bias reparameterization\n",
    "    task_type=\"CAUSAL_LM\",                   # Task type: Causal Language Modeling\n",
    "    target_modules=[\n",
    "        \"q_proj\",\n",
    "        \"k_proj\",\n",
    "        \"v_proj\",\n",
    "        \"o_proj\",\n",
    "        \"gate_proj\",\n",
    "        \"up_proj\",\n",
    "        \"down_proj\",\n",
    "    ],  # Target modules for LoRA\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-20T20:06:37.870253Z",
     "iopub.status.busy": "2025-05-20T20:06:37.869550Z",
     "iopub.status.idle": "2025-05-20T20:06:38.584061Z",
     "shell.execute_reply": "2025-05-20T20:06:38.583516Z",
     "shell.execute_reply.started": "2025-05-20T20:06:37.870226Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "model = get_peft_model(model, peft_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-20T20:06:39.153703Z",
     "iopub.status.busy": "2025-05-20T20:06:39.153119Z",
     "iopub.status.idle": "2025-05-20T20:06:39.163157Z",
     "shell.execute_reply": "2025-05-20T20:06:39.162325Z",
     "shell.execute_reply.started": "2025-05-20T20:06:39.153680Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trainable params: 52,183,040 || all params: 1,052,068,992 || trainable%: 4.9600\n"
     ]
    }
   ],
   "source": [
    "model.print_trainable_parameters()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-20T20:06:40.897761Z",
     "iopub.status.busy": "2025-05-20T20:06:40.897045Z",
     "iopub.status.idle": "2025-05-20T20:06:40.901326Z",
     "shell.execute_reply": "2025-05-20T20:06:40.900546Z",
     "shell.execute_reply.started": "2025-05-20T20:06:40.897738Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "from transformers import DataCollatorForLanguageModeling\n",
    "\n",
    "data_collator = DataCollatorForLanguageModeling(\n",
    "    tokenizer=tokenizer,\n",
    "    mlm=False\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.status.busy": "2025-05-20T20:17:32.235238Z",
     "iopub.status.idle": "2025-05-20T20:17:32.235487Z",
     "shell.execute_reply": "2025-05-20T20:17:32.235377Z",
     "shell.execute_reply.started": "2025-05-20T20:17:32.235367Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "training_arguments = TrainingArguments(\n",
    "    output_dir=\"output\",\n",
    "    per_device_train_batch_size=1,\n",
    "    per_device_eval_batch_size=1,\n",
    "    gradient_accumulation_steps=4,\n",
    "    optim=\"adamw_torch\",\n",
    "    num_train_epochs=2,\n",
    "    logging_steps=500,\n",
    "    warmup_steps=10,\n",
    "    logging_strategy=\"steps\",\n",
    "    eval_strategy=\"steps\",\n",
    "    eval_steps=1000,\n",
    "    save_steps=200,\n",
    "    save_total_limit=2,\n",
    "    learning_rate=2e-4,\n",
    "    fp16=False,\n",
    "    bf16=False,\n",
    "    max_grad_norm=0.3,\n",
    "    group_by_length=True,\n",
    "    report_to=\"none\",\n",
    "    remove_unused_columns=False,\n",
    "    label_names=[\"labels\"],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-20T20:06:41.279490Z",
     "iopub.status.busy": "2025-05-20T20:06:41.279158Z",
     "iopub.status.idle": "2025-05-20T20:06:51.238542Z",
     "shell.execute_reply": "2025-05-20T20:06:51.237693Z",
     "shell.execute_reply.started": "2025-05-20T20:06:41.279470Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e0e0d73c94cc4f76bad0972ce74925b8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/8000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "23d20cf1c17b4cebae25837671d5f8b4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/1000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def tokenize(example):\n",
    "    out = tokenizer(\n",
    "        example[\"text\"],\n",
    "        padding=\"longest\",\n",
    "        truncation=True,\n",
    "        max_length=1024,\n",
    "    )\n",
    "    out[\"labels\"] = out[\"input_ids\"].copy()\n",
    "    return out\n",
    "\n",
    "tokenized_train = train_dataset.map(tokenize, remove_columns=[\"text\"])\n",
    "tokenized_eval = eval_dataset.map(tokenize, remove_columns=[\"text\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-20T20:06:51.239991Z",
     "iopub.status.busy": "2025-05-20T20:06:51.239770Z",
     "iopub.status.idle": "2025-05-20T20:06:51.245923Z",
     "shell.execute_reply": "2025-05-20T20:06:51.245178Z",
     "shell.execute_reply.started": "2025-05-20T20:06:51.239974Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "from transformers import TrainerCallback\n",
    "import csv\n",
    "import os\n",
    "\n",
    "class LossLoggerCallback(TrainerCallback):\n",
    "    def __init__(self, log_file=\"training_logs.csv\"):\n",
    "        self.log_file = log_file\n",
    "        # создаем заголовок, если файл не существует\n",
    "        if not os.path.exists(log_file):\n",
    "            with open(log_file, \"w\", newline=\"\") as f:\n",
    "                writer = csv.writer(f)\n",
    "                writer.writerow([\"step\", \"train_loss\", \"eval_loss\"])\n",
    "\n",
    "    def on_log(self, args, state, control, logs=None, **kwargs):\n",
    "        if logs is not None:\n",
    "            with open(self.log_file, \"a\", newline=\"\") as f:\n",
    "                writer = csv.writer(f)\n",
    "                writer.writerow([\n",
    "                    state.global_step,\n",
    "                    logs.get(\"loss\", \"\"),\n",
    "                    logs.get(\"eval_loss\", \"\")\n",
    "                ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-20T20:16:31.598076Z",
     "iopub.status.busy": "2025-05-20T20:16:31.597589Z",
     "iopub.status.idle": "2025-05-20T20:16:31.638861Z",
     "shell.execute_reply": "2025-05-20T20:16:31.638307Z",
     "shell.execute_reply.started": "2025-05-20T20:16:31.598052Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "from transformers import Trainer\n",
    "loss_logger = LossLoggerCallback(log_file=\"training_logs.csv\")\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_arguments,\n",
    "    train_dataset=tokenized_train,\n",
    "    eval_dataset=tokenized_eval,\n",
    "    tokenizer=tokenizer,\n",
    "    data_collator=data_collator,\n",
    "    callbacks=[loss_logger],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-20T20:16:31.827594Z",
     "iopub.status.busy": "2025-05-20T20:16:31.827313Z",
     "iopub.status.idle": "2025-05-20T20:17:32.234823Z",
     "shell.execute_reply": "2025-05-20T20:17:32.233652Z",
     "shell.execute_reply.started": "2025-05-20T20:16:31.827574Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='18' max='50' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [18/50 00:56 < 01:52, 0.28 it/s, Epoch 0.68/2]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.027533</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>0.028800</td>\n",
       "      <td>0.029998</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>15</td>\n",
       "      <td>0.028800</td>\n",
       "      <td>0.034508</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_183/4032920361.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtrainer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/transformers/trainer.py\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(self, resume_from_checkpoint, trial, ignore_keys_for_eval, **kwargs)\u001b[0m\n\u001b[1;32m   2238\u001b[0m                 \u001b[0mhf_hub_utils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menable_progress_bars\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2239\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2240\u001b[0;31m             return inner_training_loop(\n\u001b[0m\u001b[1;32m   2241\u001b[0m                 \u001b[0margs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2242\u001b[0m                 \u001b[0mresume_from_checkpoint\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mresume_from_checkpoint\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/transformers/trainer.py\u001b[0m in \u001b[0;36m_inner_training_loop\u001b[0;34m(self, batch_size, args, resume_from_checkpoint, trial, ignore_keys_for_eval)\u001b[0m\n\u001b[1;32m   2553\u001b[0m                     )\n\u001b[1;32m   2554\u001b[0m                     \u001b[0;32mwith\u001b[0m \u001b[0mcontext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2555\u001b[0;31m                         \u001b[0mtr_loss_step\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtraining_step\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_items_in_batch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2556\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2557\u001b[0m                     if (\n",
      "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/transformers/trainer.py\u001b[0m in \u001b[0;36mtraining_step\u001b[0;34m(***failed resolving arguments***)\u001b[0m\n\u001b[1;32m   3789\u001b[0m                 \u001b[0mkwargs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"scale_wrt_gas\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3790\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3791\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maccelerator\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3792\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3793\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdetach\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/accelerate/accelerator.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, loss, **kwargs)\u001b[0m\n\u001b[1;32m   2471\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlomo_backward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlearning_rate\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2472\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2473\u001b[0;31m             \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2474\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2475\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mset_trigger\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/_tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    624\u001b[0m                 \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    625\u001b[0m             )\n\u001b[0;32m--> 626\u001b[0;31m         torch.autograd.backward(\n\u001b[0m\u001b[1;32m    627\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    628\u001b[0m         )\n",
      "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    345\u001b[0m     \u001b[0;31m# some Python versions print out the first line of a multi-line function\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    346\u001b[0m     \u001b[0;31m# calls in the traceback and some print out the last line\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 347\u001b[0;31m     _engine_run_backward(\n\u001b[0m\u001b[1;32m    348\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    349\u001b[0m         \u001b[0mgrad_tensors_\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/autograd/graph.py\u001b[0m in \u001b[0;36m_engine_run_backward\u001b[0;34m(t_outputs, *args, **kwargs)\u001b[0m\n\u001b[1;32m    821\u001b[0m         \u001b[0munregister_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_register_logging_hooks_on_whole_graph\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mt_outputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    822\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 823\u001b[0;31m         return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass\n\u001b[0m\u001b[1;32m    824\u001b[0m             \u001b[0mt_outputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    825\u001b[0m         )  # Calls into the C++ engine to run the backward pass\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "model.save_pretrained(\"gemma3:1b-text2sql-lora\")\n",
    "tokenizer.save_pretrained(\"gemma3:1b-text2sql-lora\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model.push_to_hub(\"Arsench1k/gemma3-1b-text2sql-lora\")\n",
    "# tokenizer.push_to_hub(\"Arsench1k/gemma3-1b-text2sql-lora\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "gpu",
   "dataSources": [
    {
     "datasetId": 7453378,
     "sourceId": 11861301,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 7469021,
     "sourceId": 11883830,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 7469341,
     "sourceId": 11884268,
     "sourceType": "datasetVersion"
    },
    {
     "isSourceIdPinned": false,
     "modelId": 222398,
     "modelInstanceId": 239467,
     "sourceId": 282742,
     "sourceType": "modelInstanceVersion"
    }
   ],
   "dockerImageVersionId": 31041,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "nlp",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
